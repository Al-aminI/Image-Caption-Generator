{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import torchvision.transforms as T\n",
    "from torchvision import datasets\n",
    "#from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.io import read_image\n",
    "#from IPython.display import Image\n",
    "#from pandas.core.common import flatten\n",
    "import torchvision.transforms.functional as fn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "#from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "#import albumentations as A\n",
    "#from albumentations.pytorch import ToTensorV2\n",
    "import datetime\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import imageio\n",
    "import os\n",
    "import copy\n",
    "from os import walk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>comment_number</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>Two young guys with shaggy hair look at their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Two young , White males are outside near many...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>Two men in green shirts are standing in a yard .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>A man in a blue shirt standing in a garden .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>Two friends enjoy time spent together .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158910</th>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>A man in shorts and a Hawaiian shirt leans ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158911</th>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>A young man hanging over the side of a boat ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158912</th>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>A man is leaning off of the side of a blue an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158913</th>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>A man riding a small boat in a harbor , with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158914</th>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>A man on a moored blue and white boat with hi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158915 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            image_name comment_number  \\\n",
       "0       1000092795.jpg              0   \n",
       "1       1000092795.jpg              1   \n",
       "2       1000092795.jpg              2   \n",
       "3       1000092795.jpg              3   \n",
       "4       1000092795.jpg              4   \n",
       "...                ...            ...   \n",
       "158910   998845445.jpg              0   \n",
       "158911   998845445.jpg              1   \n",
       "158912   998845445.jpg              2   \n",
       "158913   998845445.jpg              3   \n",
       "158914   998845445.jpg              4   \n",
       "\n",
       "                                                  comment  \n",
       "0        Two young guys with shaggy hair look at their...  \n",
       "1        Two young , White males are outside near many...  \n",
       "2        Two men in green shirts are standing in a yard .  \n",
       "3            A man in a blue shirt standing in a garden .  \n",
       "4                 Two friends enjoy time spent together .  \n",
       "...                                                   ...  \n",
       "158910   A man in shorts and a Hawaiian shirt leans ov...  \n",
       "158911   A young man hanging over the side of a boat ,...  \n",
       "158912   A man is leaning off of the side of a blue an...  \n",
       "158913   A man riding a small boat in a harbor , with ...  \n",
       "158914   A man on a moored blue and white boat with hi...  \n",
       "\n",
       "[158915 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('captionsfolder/results.csv', sep='|')\n",
    "#df = df.iloc[:56]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_data_preprocessor(image):\n",
    "    img_data = Image.open(\"images/flickr30k_images/\" + image)\n",
    "    image_transformer = T.Compose([\n",
    "                        T.Resize((256, 256)),\n",
    "                        #T.RandomCrop(299, 299),\n",
    "                        T.ToTensor(),\n",
    "                        T.Normalize(\n",
    "                            mean = [0.5, 0.5, 0.5],\n",
    "                            std = [0.5, 0.5, 0.5]),\n",
    "                        T.Grayscale()\n",
    "                   \n",
    "                        \n",
    "    ])\n",
    "    transformed_image_data = image_transformer(img_data)\n",
    "    return transformed_image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the captions\n",
    "vocab = set()\n",
    "for caption in df['comment']:\n",
    "    #print(caption)\n",
    "    vocab.update(caption.split())\n",
    "\n",
    "# Create a vocabulary of the most common words\n",
    "vocab = list(vocab)\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dental\n"
     ]
    }
   ],
   "source": [
    "#vocab\n",
    "word_to_idx\n",
    "print(vocab[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 7,\n",
       " 12,\n",
       " 12,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 18,\n",
       " 13,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 17,\n",
       " 15,\n",
       " 13,\n",
       " 10,\n",
       " 8,\n",
       " 20,\n",
       " 10,\n",
       " 11,\n",
       " 9,\n",
       " 7,\n",
       " 17,\n",
       " 15,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 16,\n",
       " 13,\n",
       " 11,\n",
       " 9,\n",
       " 8,\n",
       " 15,\n",
       " 14,\n",
       " 14,\n",
       " 11,\n",
       " 8,\n",
       " 17,\n",
       " 13,\n",
       " 14,\n",
       " 12,\n",
       " 7,\n",
       " 15,\n",
       " 12,\n",
       " 11,\n",
       " 10,\n",
       " 7,\n",
       " 17,\n",
       " 11,\n",
       " 11,\n",
       " 10,\n",
       " 10,\n",
       " 23,\n",
       " 16,\n",
       " 16,\n",
       " 14,\n",
       " 13,\n",
       " 19,\n",
       " 15,\n",
       " 13,\n",
       " 9,\n",
       " 9,\n",
       " 18,\n",
       " 17,\n",
       " 16,\n",
       " 13,\n",
       " 15,\n",
       " 14,\n",
       " 11,\n",
       " 12,\n",
       " 8,\n",
       " 4,\n",
       " 20,\n",
       " 17,\n",
       " 12,\n",
       " 12,\n",
       " 10,\n",
       " 21,\n",
       " 20,\n",
       " 14,\n",
       " 13,\n",
       " 10,\n",
       " 18,\n",
       " 15,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 24,\n",
       " 20,\n",
       " 11,\n",
       " 12,\n",
       " 10,\n",
       " 20,\n",
       " 14,\n",
       " 17,\n",
       " 16,\n",
       " 13,\n",
       " 14,\n",
       " 12,\n",
       " 14,\n",
       " 10,\n",
       " 12,\n",
       " 19,\n",
       " 17,\n",
       " 11,\n",
       " 9,\n",
       " 7,\n",
       " 27,\n",
       " 14,\n",
       " 12,\n",
       " 9,\n",
       " 9,\n",
       " 13,\n",
       " 11,\n",
       " 8,\n",
       " 6,\n",
       " 7,\n",
       " 20,\n",
       " 20,\n",
       " 19,\n",
       " 14,\n",
       " 16,\n",
       " 13,\n",
       " 12,\n",
       " 12,\n",
       " 10,\n",
       " 9,\n",
       " 29,\n",
       " 22,\n",
       " 19,\n",
       " 16,\n",
       " 9,\n",
       " 14,\n",
       " 12,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 12,\n",
       " 13,\n",
       " 12,\n",
       " 8,\n",
       " 8,\n",
       " 14,\n",
       " 15,\n",
       " 9,\n",
       " 11,\n",
       " 9,\n",
       " 18,\n",
       " 16,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 19,\n",
       " 16,\n",
       " 14,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 15,\n",
       " 11,\n",
       " 11,\n",
       " 6,\n",
       " 8,\n",
       " 17,\n",
       " 16,\n",
       " 11,\n",
       " 12,\n",
       " 9,\n",
       " 17,\n",
       " 16,\n",
       " 12,\n",
       " 10,\n",
       " 11,\n",
       " 40,\n",
       " 12,\n",
       " 12,\n",
       " 10,\n",
       " 8,\n",
       " 22,\n",
       " 23,\n",
       " 14,\n",
       " 13,\n",
       " 11,\n",
       " 17,\n",
       " 14,\n",
       " 13,\n",
       " 10,\n",
       " 9,\n",
       " 22,\n",
       " 19,\n",
       " 18,\n",
       " 7,\n",
       " 8,\n",
       " 21,\n",
       " 23,\n",
       " 16,\n",
       " 6,\n",
       " 6,\n",
       " 35,\n",
       " 21,\n",
       " 17,\n",
       " 8,\n",
       " 6,\n",
       " 20,\n",
       " 12,\n",
       " 8,\n",
       " 9,\n",
       " 6,\n",
       " 15,\n",
       " 13,\n",
       " 12,\n",
       " 6,\n",
       " 6,\n",
       " 24,\n",
       " 18,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 17,\n",
       " 12,\n",
       " 9,\n",
       " 10,\n",
       " 5,\n",
       " 14,\n",
       " 13,\n",
       " 12,\n",
       " 10,\n",
       " 9,\n",
       " 15,\n",
       " 13,\n",
       " 15,\n",
       " 14,\n",
       " 16,\n",
       " 14,\n",
       " 12,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 24,\n",
       " 17,\n",
       " 11,\n",
       " 11,\n",
       " 8,\n",
       " 31,\n",
       " 17,\n",
       " 14,\n",
       " 15,\n",
       " 9,\n",
       " 18,\n",
       " 15,\n",
       " 16,\n",
       " 13,\n",
       " 8,\n",
       " 21,\n",
       " 16,\n",
       " 13,\n",
       " 12,\n",
       " 10,\n",
       " 21,\n",
       " 18,\n",
       " 17,\n",
       " 15,\n",
       " 10,\n",
       " 20,\n",
       " 20,\n",
       " 17,\n",
       " 14,\n",
       " 12,\n",
       " 20,\n",
       " 18,\n",
       " 13,\n",
       " 8,\n",
       " 6,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 13,\n",
       " 8,\n",
       " 16,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 22,\n",
       " 17,\n",
       " 18,\n",
       " 11,\n",
       " 11,\n",
       " 17,\n",
       " 14,\n",
       " 13,\n",
       " 11,\n",
       " 10,\n",
       " 21,\n",
       " 22,\n",
       " 16,\n",
       " 16,\n",
       " 11,\n",
       " 14,\n",
       " 13,\n",
       " 11,\n",
       " 6,\n",
       " 7,\n",
       " 13,\n",
       " 11,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 14,\n",
       " 17,\n",
       " 12,\n",
       " 13,\n",
       " 11,\n",
       " 20,\n",
       " 15,\n",
       " 13,\n",
       " 11,\n",
       " 9,\n",
       " 17,\n",
       " 14,\n",
       " 13,\n",
       " 12,\n",
       " 9,\n",
       " 20,\n",
       " 12,\n",
       " 11,\n",
       " 10,\n",
       " 8,\n",
       " 15,\n",
       " 13,\n",
       " 15,\n",
       " 10,\n",
       " 8,\n",
       " 28,\n",
       " 20,\n",
       " 14,\n",
       " 17,\n",
       " 13,\n",
       " 20,\n",
       " 17,\n",
       " 12,\n",
       " 12,\n",
       " 8,\n",
       " 12,\n",
       " 13,\n",
       " 11,\n",
       " 8,\n",
       " 8,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 13,\n",
       " 9,\n",
       " 15,\n",
       " 15,\n",
       " 14,\n",
       " 12,\n",
       " 8,\n",
       " 26,\n",
       " 15,\n",
       " 14,\n",
       " 14,\n",
       " 12,\n",
       " 16,\n",
       " 14,\n",
       " 14,\n",
       " 12,\n",
       " 11,\n",
       " 15,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 22,\n",
       " 21,\n",
       " 18,\n",
       " 9,\n",
       " 8,\n",
       " 20,\n",
       " 17,\n",
       " 10,\n",
       " 8,\n",
       " 7,\n",
       " 23,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 8,\n",
       " 19,\n",
       " 18,\n",
       " 18,\n",
       " 14,\n",
       " 10,\n",
       " 22,\n",
       " 17,\n",
       " 14,\n",
       " 11,\n",
       " 12,\n",
       " 17,\n",
       " 14,\n",
       " 13,\n",
       " 11,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 12,\n",
       " 12,\n",
       " 8,\n",
       " 9,\n",
       " 6,\n",
       " 13,\n",
       " 12,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 17,\n",
       " 12,\n",
       " 11,\n",
       " 8,\n",
       " 5,\n",
       " 15,\n",
       " 11,\n",
       " 11,\n",
       " 10,\n",
       " 10,\n",
       " 16,\n",
       " 16,\n",
       " 13,\n",
       " 13,\n",
       " 8,\n",
       " 17,\n",
       " 16,\n",
       " 15,\n",
       " 8,\n",
       " 7,\n",
       " 55,\n",
       " 18,\n",
       " 17,\n",
       " 10,\n",
       " 6,\n",
       " 17,\n",
       " 13,\n",
       " 9,\n",
       " 12,\n",
       " 11,\n",
       " 17,\n",
       " 12,\n",
       " 11,\n",
       " 8,\n",
       " 7,\n",
       " 24,\n",
       " 20,\n",
       " 17,\n",
       " 18,\n",
       " 9,\n",
       " 25,\n",
       " 20,\n",
       " 17,\n",
       " 10,\n",
       " 7,\n",
       " 16,\n",
       " 13,\n",
       " 9,\n",
       " 11,\n",
       " 6,\n",
       " 32,\n",
       " 21,\n",
       " 20,\n",
       " 16,\n",
       " 13,\n",
       " 11,\n",
       " 11,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 17,\n",
       " 14,\n",
       " 13,\n",
       " 9,\n",
       " 8,\n",
       " 40,\n",
       " 37,\n",
       " 18,\n",
       " 20,\n",
       " 16,\n",
       " 14,\n",
       " 10,\n",
       " 11,\n",
       " 9,\n",
       " 7,\n",
       " 16,\n",
       " 15,\n",
       " 14,\n",
       " 11,\n",
       " 11,\n",
       " 20,\n",
       " 15,\n",
       " 11,\n",
       " 12,\n",
       " 6,\n",
       " 27,\n",
       " 16,\n",
       " 10,\n",
       " 12,\n",
       " 9,\n",
       " 26,\n",
       " 18,\n",
       " 19,\n",
       " 15,\n",
       " 10,\n",
       " 20,\n",
       " 14,\n",
       " 13,\n",
       " 10,\n",
       " 9,\n",
       " 20,\n",
       " 19,\n",
       " 16,\n",
       " 16,\n",
       " 13,\n",
       " 19,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 11,\n",
       " 19,\n",
       " 19,\n",
       " 16,\n",
       " 15,\n",
       " 8,\n",
       " 24,\n",
       " 19,\n",
       " 15,\n",
       " 12,\n",
       " 9,\n",
       " 17,\n",
       " 16,\n",
       " 13,\n",
       " 14,\n",
       " 10,\n",
       " 15,\n",
       " 13,\n",
       " 12,\n",
       " 10,\n",
       " 6,\n",
       " 17,\n",
       " 12,\n",
       " 11,\n",
       " 9,\n",
       " 9,\n",
       " 21,\n",
       " 13,\n",
       " 14,\n",
       " 10,\n",
       " 9,\n",
       " 17,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 6,\n",
       " 26,\n",
       " 16,\n",
       " 10,\n",
       " 8,\n",
       " 5,\n",
       " 12,\n",
       " 12,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 14,\n",
       " 13,\n",
       " 12,\n",
       " 9,\n",
       " 8,\n",
       " 23,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 6,\n",
       " 20,\n",
       " 20,\n",
       " 17,\n",
       " 15,\n",
       " 13,\n",
       " 15,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 10,\n",
       " 26,\n",
       " 18,\n",
       " 19,\n",
       " 14,\n",
       " 12,\n",
       " 22,\n",
       " 16,\n",
       " 15,\n",
       " 8,\n",
       " 6,\n",
       " 26,\n",
       " 19,\n",
       " 17,\n",
       " 12,\n",
       " 9,\n",
       " 16,\n",
       " 11,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 29,\n",
       " 26,\n",
       " 23,\n",
       " 17,\n",
       " 12,\n",
       " 15,\n",
       " 11,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 32,\n",
       " 13,\n",
       " 11,\n",
       " 9,\n",
       " 6,\n",
       " 11,\n",
       " 11,\n",
       " 10,\n",
       " 11,\n",
       " 7,\n",
       " 19,\n",
       " 15,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 18,\n",
       " 17,\n",
       " 14,\n",
       " 10,\n",
       " 9,\n",
       " 25,\n",
       " 11,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 24,\n",
       " 16,\n",
       " 14,\n",
       " 14,\n",
       " 7,\n",
       " 25,\n",
       " 19,\n",
       " 15,\n",
       " 15,\n",
       " 7,\n",
       " 44,\n",
       " 31,\n",
       " 17,\n",
       " 11,\n",
       " 7,\n",
       " 38,\n",
       " 29,\n",
       " 24,\n",
       " 16,\n",
       " 14,\n",
       " 23,\n",
       " 20,\n",
       " 16,\n",
       " 14,\n",
       " 10,\n",
       " 14,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 13,\n",
       " 11,\n",
       " 12,\n",
       " 7,\n",
       " 7,\n",
       " 26,\n",
       " 15,\n",
       " 14,\n",
       " 13,\n",
       " 12,\n",
       " 16,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 7,\n",
       " 20,\n",
       " 14,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 10,\n",
       " 11,\n",
       " 10,\n",
       " 8,\n",
       " 6,\n",
       " 18,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 15,\n",
       " 18,\n",
       " 15,\n",
       " 11,\n",
       " 8,\n",
       " 7,\n",
       " 24,\n",
       " 19,\n",
       " 15,\n",
       " 9,\n",
       " 8,\n",
       " 30,\n",
       " 21,\n",
       " 18,\n",
       " 17,\n",
       " 13,\n",
       " 27,\n",
       " 14,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 21,\n",
       " 21,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 19,\n",
       " 19,\n",
       " 16,\n",
       " 15,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 7,\n",
       " 31,\n",
       " 23,\n",
       " 21,\n",
       " 12,\n",
       " 12,\n",
       " 16,\n",
       " 11,\n",
       " 12,\n",
       " 10,\n",
       " 7,\n",
       " 17,\n",
       " 16,\n",
       " 12,\n",
       " 10,\n",
       " 7,\n",
       " 40,\n",
       " 34,\n",
       " 18,\n",
       " 14,\n",
       " 12,\n",
       " 22,\n",
       " 24,\n",
       " 19,\n",
       " 17,\n",
       " 14,\n",
       " 20,\n",
       " 18,\n",
       " 16,\n",
       " 13,\n",
       " 8,\n",
       " 17,\n",
       " 18,\n",
       " 16,\n",
       " 12,\n",
       " 3,\n",
       " 18,\n",
       " 14,\n",
       " 13,\n",
       " 8,\n",
       " 8,\n",
       " 18,\n",
       " 17,\n",
       " 13,\n",
       " 11,\n",
       " 11,\n",
       " 15,\n",
       " 13,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 14,\n",
       " 13,\n",
       " 9,\n",
       " 7,\n",
       " 6,\n",
       " 13,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 25,\n",
       " 25,\n",
       " 22,\n",
       " 10,\n",
       " 7,\n",
       " 15,\n",
       " 11,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 12,\n",
       " 14,\n",
       " 15,\n",
       " 10,\n",
       " 10,\n",
       " 12,\n",
       " 12,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 13,\n",
       " 11,\n",
       " 11,\n",
       " 8,\n",
       " 8,\n",
       " 12,\n",
       " 9,\n",
       " 11,\n",
       " 10,\n",
       " 9,\n",
       " 21,\n",
       " 13,\n",
       " 10,\n",
       " 8,\n",
       " 7,\n",
       " 18,\n",
       " 15,\n",
       " 14,\n",
       " 13,\n",
       " 11,\n",
       " 14,\n",
       " 13,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 15,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 23,\n",
       " 18,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 16,\n",
       " 14,\n",
       " 11,\n",
       " 10,\n",
       " 7,\n",
       " 14,\n",
       " 13,\n",
       " 12,\n",
       " 13,\n",
       " 12,\n",
       " 11,\n",
       " 13,\n",
       " 11,\n",
       " 11,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 18,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 9,\n",
       " 19,\n",
       " 15,\n",
       " 13,\n",
       " 12,\n",
       " 8,\n",
       " 26,\n",
       " 20,\n",
       " 19,\n",
       " 15,\n",
       " 14,\n",
       " 19,\n",
       " 13,\n",
       " 12,\n",
       " 10,\n",
       " 8,\n",
       " 15,\n",
       " 17,\n",
       " 16,\n",
       " 14,\n",
       " 13,\n",
       " 17,\n",
       " 16,\n",
       " 13,\n",
       " 13,\n",
       " 10,\n",
       " 12,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 19,\n",
       " 11,\n",
       " 14,\n",
       " 12,\n",
       " 10,\n",
       " 23,\n",
       " 12,\n",
       " 9,\n",
       " 8,\n",
       " 6,\n",
       " 33,\n",
       " 19,\n",
       " 11,\n",
       " 10,\n",
       " 9,\n",
       " 24,\n",
       " 12,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 21,\n",
       " 18,\n",
       " 17,\n",
       " 16,\n",
       " 11,\n",
       " 34,\n",
       " 24,\n",
       " 12,\n",
       " 10,\n",
       " 6,\n",
       " 24,\n",
       " 18,\n",
       " 16,\n",
       " 15,\n",
       " 11,\n",
       " 16,\n",
       " 11,\n",
       " 11,\n",
       " 10,\n",
       " 6,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 8,\n",
       " 7,\n",
       " 22,\n",
       " 18,\n",
       " 15,\n",
       " 12,\n",
       " 11,\n",
       " 18,\n",
       " 15,\n",
       " 14,\n",
       " 11,\n",
       " 8,\n",
       " 23,\n",
       " 13,\n",
       " 13,\n",
       " 9,\n",
       " 9,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 12,\n",
       " 13,\n",
       " 20,\n",
       " 12,\n",
       " 10,\n",
       " 9,\n",
       " 5,\n",
       " 20,\n",
       " 14,\n",
       " 11,\n",
       " 11,\n",
       " 8,\n",
       " 23,\n",
       " 14,\n",
       " 12,\n",
       " 13,\n",
       " 11,\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map each word in the captions to an integer index\n",
    "captions = []\n",
    "for caption in df['comment']:\n",
    "    tokens = caption.split()\n",
    "    indices = [word_to_idx[token] for token in tokens]\n",
    "    captions.append(indices)\n",
    " \n",
    "\n",
    "lengths_of_captions = []\n",
    "for caption_data in captions:\n",
    "    lengths_of_captions.append(len(caption_data))\n",
    "lengths_of_captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_length = max(lengths_of_captions)\n",
    "captions_arrays = [0] * 256\n",
    "list_of_captions_arrays = []\n",
    "\n",
    "for array in captions:\n",
    "    captions_arrays[:len(array)] = array\n",
    "    new_caption_list = captions_arrays\n",
    "    #print(new_caption_list)\n",
    "    list_of_captions_arrays.append(list(new_caption_list))\n",
    "\n",
    "#print(list_of_captions_arrays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_tensor = torch.tensor(list_of_captions_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19621, 21030,   603,  ...,     0,     0,     0],\n",
       "        [19621, 21030,  5957,  ...,     0,     0,     0],\n",
       "        [19621, 21720,  5695,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [19441, 18542,  5756,  ...,     0,     0,     0],\n",
       "        [19441, 18542,  5251,  ...,     0,     0,     0],\n",
       "        [19441, 18542, 18146,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, images, captions):\n",
    "        self.images = images\n",
    "        self.captions = captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        caption = self.captions[idx]\n",
    "        return image, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CaptionDataset(df['image_name'], captions_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, caption in dataset:\n",
    "    image = image_data_preprocessor(image)\n",
    "    plt.imshow(image[0])\n",
    "    plt.show()\n",
    "    #print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size = 10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, embed_s):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        \"\"\"self.train_CNN = train_CNN\n",
    "        self.inception = models.inception_v3(pretrained = True, aux_logits = True)\n",
    "        self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_s)\n",
    "        self.cnn = torch.nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout(0.5),\n",
    "            #nn.Flatten()\n",
    "        )\"\"\"\n",
    "        self.cnn = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "            #nn.MaxPool2d(kernel_size=2),\n",
    "        self.drp = nn.Dropout(0.5)\n",
    "            \n",
    "       \n",
    "        #self.dropout = nn.Dropout(0.5)\n",
    "        self.attention = nn.Linear(embed_s, embed_s)\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "        #self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_img):\n",
    "        #print(input_img.shape)\n",
    "        output_features = self.drp(self.relu(self.cnn(input_img)))\n",
    "        #print(output_features.shape)\n",
    "        #output_features = self.cnn(input_img)\n",
    "        attention = self.attention(output_features)\n",
    "        #feature = self.inception(input_img.unsqueeze(0))\n",
    "        #attention_output = self.softmax(attention)\n",
    "        #feature_embedding = output_features * attention_output\n",
    "        \"\"\"for name, param in self.inception.named_parameters():\n",
    "            if \"fc_weight\" in name or \"fc_bias\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = self.train_CNN\"\"\"\n",
    "\n",
    "        return attention\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_s, hidden_s, vocab_s, num_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_s = hidden_s\n",
    "        self.embed = nn.Embedding(vocab_s, embed_s)\n",
    "        self.lstm = nn.LSTM(embed_s, hidden_s, num_layers)\n",
    "        self.linear = nn.Linear(hidden_s, vocab_s)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        #self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        #h_0 = torch.zeros()\n",
    "        embeddings = self.embed(captions)\n",
    "        #embeddings = embeddings.detach().requires_grad_(True)\n",
    "        #print( features.unsqueeze(0).shape, embeddings.shape)\n",
    "        concatenated_embeddings = torch.cat((features, embeddings),dim = 0)\n",
    "        #print(concatenated_embeddings.shape)\n",
    "        hidden, _ = self.lstm(concatenated_embeddings)\n",
    "        #print(hidden.shape)\n",
    "        #hidden = hidden.view(-1, self.hidden_s)\n",
    "        #print(hidden.shape)\n",
    "        #hidden.requires_grad = True\n",
    "        output = self.linear(hidden)\n",
    "        #print(output.shape)\n",
    "        #output = self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Decoder(nn.Module):\n",
    "    def __init__(self, embed_s, hidden_s, vocab_s, num_layers):\n",
    "        super(Encoder_Decoder, self).__init__()\n",
    "        self.encoder = CNNEncoder(embed_s)\n",
    "        self.decoder = DecoderRNN(embed_s, hidden_s, vocab_s, num_layers)\n",
    "    \n",
    "    def forward(self, images, captions):\n",
    "        #print(image.unsqueeze(0).shape)\n",
    "        features = self.encoder(images)\n",
    "        #print(features.shape, captions.shape)\n",
    "        output = self.decoder(features, captions)\n",
    "        #print(output.shape)\n",
    "        return output\n",
    "\n",
    "    \"\"\" def caption_image(self, image, vocabulary, max_l = 256):\n",
    "        result_cap =[]\n",
    "        with torch.no_grad():\n",
    "            x = self.encoder(image).squeeze(0)\n",
    "            states = None\n",
    "\n",
    "            for _ in range(maximum_l):\n",
    "                hiddens, states = self.decoder.lstm(x, states)\n",
    "                output = self.decoder.linear(hiddens.unsqueeze(0))\n",
    "                predicted = output.argmax(1)\n",
    "\n",
    "                result_cap.append(predicted.item())\n",
    "                x = self.decoder.embed(predicted).unsqueeze(0)\n",
    "\n",
    "                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
    "                    break\n",
    "\n",
    "        return [vocabulary.itos[idx] for idx in result_cap]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 256\n",
    "hidden_size = 256\n",
    "vocab_size = len(vocab)\n",
    "num_layers = 1\n",
    "learning_rate = 0.01\n",
    "num_epochs = 500\n",
    "encoder = CNNEncoder(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "model = Encoder_Decoder(embed_size, hidden_size, vocab_size, num_layers)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "#loss_function = loss_function.requires_grad_(True)\n",
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=learning_rate)#, alpha= 0.99, eps = 1e-08)\n",
    "#torch.set_grad_enabled(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 256])\n",
      "2023-01-14 21:29:21.528162 Epoch 1, Training loss 0.000639862837792643\n",
      "torch.Size([9, 256])\n",
      "2023-01-14 21:29:23.750221 Epoch 1, Training loss 0.0006397465989837532\n",
      "torch.Size([9, 256])\n",
      "2023-01-14 21:29:26.004192 Epoch 1, Training loss 0.0006397247554491966\n",
      "torch.Size([9, 256])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[39m#print(output.shape, caption.shape)\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m loss \u001b[39m=\u001b[39m loss_function(output, caption)\n\u001b[0;32m     21\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     22\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Al-amin I\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Al-amin I\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32mc:\\Users\\Al-amin I\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:3026\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3024\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3025\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3026\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#model.eval()\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    \n",
    "    for i, (image, caption) in enumerate(train_dataloader):\n",
    "        #print(image)\n",
    "        optimizer.zero_grad()\n",
    "        #image = image_data_preprocessor(image)\n",
    "        image = image_data_preprocessor(str(image[0]))\n",
    "        #image = image.to(device)\n",
    "        #caption = caption.to(device)\n",
    "        print(caption[:-1].shape)\n",
    "        output = model(image, caption[:-1])\n",
    "        \n",
    "        #output = torch.tensor(output, requires_grad=True)\n",
    "        #print(output.shape, caption.shape)\n",
    "        #max_output = output.argmax(dim=2)\n",
    "        output = output.permute(0, 2, 1)\n",
    "        #print(output.shape, caption.shape)\n",
    "        loss = loss_function(output, caption)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_train = loss.item()\n",
    "        #if epoch == 1 or epoch % 10 == 0:\n",
    "        print('{} Epoch {}, Training loss {}'.format(\n",
    "        datetime.datetime.now(), epoch,\n",
    "        loss_train/len(train_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder_Decoder(\n",
       "  (encoder): CNNEncoder(\n",
       "    (cnn): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu): ReLU()\n",
       "    (drp): Dropout(p=0.5, inplace=False)\n",
       "    (attention): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (decoder): DecoderRNN(\n",
       "    (embed): Embedding(220, 256)\n",
       "    (lstm): LSTM(256, 256)\n",
       "    (linear): Linear(in_features=256, out_features=220, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (softmax): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_captions = []\n",
    "\n",
    "for i, (image, caption) in enumerate(train_dataloader): \n",
    "        image = image_data_preprocessor(str(image[0]))\n",
    "        predi = model(image, caption)\n",
    "        preds = nn.Softmax(dim=1)\n",
    "        pred = preds(predi[0])\n",
    "        each_ind = pred.max(dim=1)\n",
    "        each_seq = each_ind[1]\n",
    "        predicted_captions.append(each_ind[1])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_new_array = []\n",
    "second_new_array = []\n",
    "for i, image_tensor in enumerate(predicted_captions):\n",
    "        first_new_array[:23] = predicted_captions[i][:23]\n",
    "        second_new_array.append(first_new_array)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(141),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(137),\n",
       " tensor(58),\n",
       " tensor(0),\n",
       " tensor(58),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(58),\n",
       " tensor(141),\n",
       " tensor(67),\n",
       " tensor(0),\n",
       " tensor(141),\n",
       " tensor(141),\n",
       " tensor(23),\n",
       " tensor(137),\n",
       " tensor(137),\n",
       " tensor(141),\n",
       " tensor(58),\n",
       " tensor(52),\n",
       " tensor(58)]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_new_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "second_sentence = []\n",
    "for i in range(len(second_new_array)):\n",
    "    first_sentence = []\n",
    "    for ind in second_new_array[i]:\n",
    "        word = vocab[ind]\n",
    "        first_sentence.append(word)\n",
    "    second_sentence.append(first_sentence)\n",
    "        #print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stands', 'friends', 'stands', 'bushes', 'to', 'walking', 'look', 'cleaning', 'gate', 'Three', 'gate', 'outside', 'outside', 'Several', 'hip', 'talking', 'street', 'outside', 'out', 'suit', 'dressed', 'outside', 'past']\n",
      " Two young guys with shaggy hair look at their hands while hanging out in the yard .\n",
      " Two young , White males are outside near many bushes .\n",
      " Two men in green shirts are standing in a yard .\n",
      " A man in a blue shirt standing in a garden .\n",
      " Two friends enjoy time spent together .\n",
      " Several men in hard hats are operating a giant pulley system .\n",
      " Workers look down from up above on a piece of equipment .\n",
      " Two men working on a machine wearing hard hats .\n",
      " Four men on top of a tall structure .\n",
      " Three men on a large rig .\n",
      " A child in a pink dress is climbing up a set of stairs in an entry way .\n",
      " A little girl in a pink dress going into a wooden cabin .\n",
      " A little girl climbing the stairs to her playhouse .\n",
      " A little girl climbing into a wooden playhouse \n",
      " A girl going into a wooden building .\n",
      " Someone in a blue shirt and hat is standing on stair and leaning against a window .\n",
      " A man in a blue shirt is standing on a ladder cleaning a window .\n",
      " A man on a ladder cleans the window of a tall building .\n",
      " man in blue shirt and jeans on ladder cleaning windows\n",
      " a man on a ladder cleans a window\n",
      " Two men , one in a gray shirt , one in a black shirt , standing near a stove .\n",
      " Two guy cooking and joking around with the camera .\n",
      " Two men in a kitchen cooking food on a stove .\n",
      " Two men are at the stove preparing food .\n",
      " Two men are cooking a meal .\n",
      " Two people in the photo are playing the guitar and the other is poking at him .\n",
      " A man in green holds a guitar while the other man observes his shirt .\n",
      " A man is fixing the guitar players costume .\n",
      " a guy stitching up another man 's coat .\n",
      " the two boys playing guitar\n",
      " A man sits in a chair while holding a large stuffed animal of a lion .\n",
      " A man is sitting on a chair holding a large stuffed animal .\n",
      " A man completes the finishing touches on a stuffed lion .\n",
      " A man holds a large stuffed lion toy .\n",
      " A man is smiling at a stuffed lion\n",
      " A girl is on rollerskates talking on her cellphone standing in a parking lot .\n",
      " A trendy girl talking on her cellphone while gliding slowly down the street .\n",
      " A young adult wearing rollerblades , holding a cellular phone to her ear .\n",
      " there is a young girl on her cellphone while skating .\n",
      " Woman talking on cellphone and wearing rollerskates .\n",
      " An asian man wearing a black suit stands near a dark-haired woman and a brown-haired woman .\n",
      " Three people are standing outside near large pipes and a metal railing .\n",
      " A young woman walks past two young people dressed in hip black outfits .\n",
      " A woman with a large purse is walking by a gate .\n",
      " Several people standing outside a building .\n",
      " Two men in Germany jumping over a rail at the same time without shirts .\n",
      " Two youths are jumping over a roadside railing , at night .\n",
      " Boys dancing on poles in the middle of the night .\n",
      " Two men with no shirts jumping over a rail .\n",
      " two guys jumping over a gate together\n",
      " Five ballet dancers caught mid jump in a dancing studio with sunlight coming through a window .\n",
      " Ballet dancers in a studio practice jumping with wonderful form .\n",
      " Five girls are leaping simultaneously in a dance practice room .\n",
      " Five girls dancing and bending feet in ballet class .\n",
      " A ballet class of five girls jumping in sequence .\n",
      " Three young men and a young woman wearing sneakers are leaping in midair at the top of a flight of concrete stairs .\n"
     ]
    }
   ],
   "source": [
    "print(second_sentence[0])\n",
    "for cap in df['comment']:\n",
    "    print(cap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predicted_captions[0][0]\n",
    "new_array = []\n",
    "new_array[:23] = predicted_captions[0][:23]\n",
    "len(new_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = []\n",
    "for i in new_array:\n",
    "    word = vocab[i]\n",
    "    sentence.append(word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec74cdd69a60fd375c07d7fa8348a6e5879c95fafc30c720580cbbe500fa6c2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
